{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  ? mcgrath rentcorp said as a result of its dec...      3\n",
      "1  ? generale de banque sa lt genb br and lt hell...      4\n",
      "2  ? shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlr...      3\n",
      "3  ? the farmers home administration the u s agri...      4\n",
      "4  ? seton co said its board has received a propo...      4\n"
     ]
    }
   ],
   "source": [
    "# Load the word index to decode the sequences back to text\n",
    "word_index = reuters.get_word_index()\n",
    "\n",
    "# Inverting the word index dictionary to decode indices back to words\n",
    "index_to_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Decoding function\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([index_to_word.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "# Decode the train and test data\n",
    "train_texts = [decode_review(sequence) for sequence in train_data]\n",
    "test_texts = [decode_review(sequence) for sequence in test_data]\n",
    "\n",
    "# Create pandas DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': test_texts,\n",
    "    \n",
    "})\n",
    "\n",
    "# Display the first few rows of the train DataFrame\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>? mcgrath rentcorp said as a result of its dec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>? generale de banque sa lt genb br and lt hell...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>? shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>? the farmers home administration the u s agri...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>? seton co said its board has received a propo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>? finance minister kiichi miyazawa said japan ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>? the price of gold bullion is likely to rise ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>? international phoenix energy corp said it la...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>? shr loss 76 cts vs profit 50 cts net loss 19...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>? currency fluctuations may reassert their inf...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     ? mcgrath rentcorp said as a result of its dec...      3\n",
       "1     ? generale de banque sa lt genb br and lt hell...      4\n",
       "2     ? shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlr...      3\n",
       "3     ? the farmers home administration the u s agri...      4\n",
       "4     ? seton co said its board has received a propo...      4\n",
       "...                                                 ...    ...\n",
       "8977  ? finance minister kiichi miyazawa said japan ...     19\n",
       "8978  ? the price of gold bullion is likely to rise ...     19\n",
       "8979  ? international phoenix energy corp said it la...     25\n",
       "8980  ? shr loss 76 cts vs profit 50 cts net loss 19...      3\n",
       "8981  ? currency fluctuations may reassert their inf...     25\n",
       "\n",
       "[8982 rows x 2 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "train_df['cleaned_text'] = train_df['text'].apply(remove_stopwords_and_punctuation)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(remove_stopwords_and_punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [mcgrath, rentcorp, said, result, december, ac...\n",
       "1       [generale, de, banque, sa, lt, genb, br, lt, h...\n",
       "2       [shr, 3, 28, dlrs, vs, 22, cts, shr, diluted, ...\n",
       "3       [farmers, home, administration, u, agriculture...\n",
       "4       [seton, co, said, board, received, proposal, c...\n",
       "                              ...                        \n",
       "8977    [finance, minister, kiichi, miyazawa, said, ja...\n",
       "8978    [price, gold, bullion, likely, rise, second, h...\n",
       "8979    [international, phoenix, energy, corp, said, l...\n",
       "8980    [shr, loss, 76, cts, vs, profit, 50, cts, net,...\n",
       "8981    [currency, fluctuations, may, reassert, influe...\n",
       "Name: cleaned_text, Length: 8982, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [great, atlantic, pacific, tea, co, said, thre...\n",
       "1       [philippine, sugar, production, 1987, 88, crop...\n",
       "2       [agriculture, departments, widening, louisiana...\n",
       "3       [graham, mccormick, oil, gas, partnership, sai...\n",
       "4       [strong, south, easterly, winds, keeping, many...\n",
       "                              ...                        \n",
       "2241    [lt, toronto, sun, publishing, corp, said, pla...\n",
       "2242    [shr, loss, 45, cts, vs, profit, 20, cts, net,...\n",
       "2243    [sept, 30, shr, 44, cts, net, 905, 000, vs, 63...\n",
       "2244    [inland, vacuum, inc, said, board, proposed, t...\n",
       "2245    [congress, give, u, agriculture, secretary, au...\n",
       "Name: cleaned_text, Length: 2246, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##skip gram\n",
    "sentences = train_df['cleaned_text'].tolist()\n",
    "sentences = test_df['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    sg=1,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.save(\"skipgram_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "loaded_model=loaded_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def skip_embedding(words):\n",
    "    embedding = [loaded_model[word] for word in words if word in loaded_model]\n",
    "    return np.mean(embedding, axis=0)\n",
    "\n",
    "embedding= train_df['cleaned_text'].apply(skip_embedding)  \n",
    "embedding1 = test_df['cleaned_text'].apply(skip_embedding)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data1=pd.DataFrame(embedding1.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 100)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape   ##train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 100)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape ##test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 46)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to_categorical \n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_df['label'])\n",
    "train_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Checkpoints -- Saves the best MOdel\n",
    "model_name = \"best1.keras\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(100,)),  # Input layer\n",
    "    Dense(64, activation='relu'),                            # Hidden layer 1\n",
    "    Dropout(0.1),                                            # Dropout layer \n",
    "    Dense(16, activation='relu'),   \n",
    "    Dense(46, activation='softmax')                           # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m48/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1821 - loss: 3.5262 \n",
      "Epoch 1: val_loss improved from inf to 2.15792, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.2038 - loss: 3.4609 - val_accuracy: 0.5081 - val_loss: 2.1579 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m49/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4925 - loss: 2.1069\n",
      "Epoch 2: val_loss improved from 2.15792 to 1.73166, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4943 - loss: 2.0900 - val_accuracy: 0.5754 - val_loss: 1.7317 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5613 - loss: 1.7774\n",
      "Epoch 3: val_loss improved from 1.73166 to 1.64044, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5649 - loss: 1.7668 - val_accuracy: 0.5943 - val_loss: 1.6404 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5995 - loss: 1.6627\n",
      "Epoch 4: val_loss improved from 1.64044 to 1.56788, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6001 - loss: 1.6586 - val_accuracy: 0.6450 - val_loss: 1.5679 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6212 - loss: 1.6025\n",
      "Epoch 5: val_loss improved from 1.56788 to 1.50128, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6214 - loss: 1.6022 - val_accuracy: 0.6533 - val_loss: 1.5013 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6332 - loss: 1.5608\n",
      "Epoch 6: val_loss improved from 1.50128 to 1.42766, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6370 - loss: 1.5493 - val_accuracy: 0.6661 - val_loss: 1.4277 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6511 - loss: 1.5028\n",
      "Epoch 7: val_loss improved from 1.42766 to 1.38016, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6534 - loss: 1.4837 - val_accuracy: 0.6756 - val_loss: 1.3802 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6511 - loss: 1.4418\n",
      "Epoch 8: val_loss improved from 1.38016 to 1.33552, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6543 - loss: 1.4320 - val_accuracy: 0.6750 - val_loss: 1.3355 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6546 - loss: 1.3873\n",
      "Epoch 9: val_loss improved from 1.33552 to 1.29498, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6572 - loss: 1.3821 - val_accuracy: 0.6811 - val_loss: 1.2950 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6732 - loss: 1.3303\n",
      "Epoch 10: val_loss improved from 1.29498 to 1.25940, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6729 - loss: 1.3303 - val_accuracy: 0.6873 - val_loss: 1.2594 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6845 - loss: 1.2835\n",
      "Epoch 11: val_loss improved from 1.25940 to 1.22905, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6837 - loss: 1.2861 - val_accuracy: 0.6934 - val_loss: 1.2291 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6999 - loss: 1.2407\n",
      "Epoch 12: val_loss improved from 1.22905 to 1.22223, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6983 - loss: 1.2465 - val_accuracy: 0.6967 - val_loss: 1.2222 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7001 - loss: 1.2189\n",
      "Epoch 13: val_loss improved from 1.22223 to 1.20021, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6997 - loss: 1.2247 - val_accuracy: 0.7012 - val_loss: 1.2002 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6931 - loss: 1.2431\n",
      "Epoch 14: val_loss improved from 1.20021 to 1.17613, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6952 - loss: 1.2394 - val_accuracy: 0.7034 - val_loss: 1.1761 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6971 - loss: 1.2283\n",
      "Epoch 15: val_loss improved from 1.17613 to 1.16216, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6973 - loss: 1.2276 - val_accuracy: 0.7073 - val_loss: 1.1622 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7035 - loss: 1.2206\n",
      "Epoch 16: val_loss improved from 1.16216 to 1.15397, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7044 - loss: 1.2109 - val_accuracy: 0.7129 - val_loss: 1.1540 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7111 - loss: 1.1700\n",
      "Epoch 17: val_loss improved from 1.15397 to 1.15003, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7111 - loss: 1.1701 - val_accuracy: 0.7123 - val_loss: 1.1500 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7012 - loss: 1.2042\n",
      "Epoch 18: val_loss improved from 1.15003 to 1.13123, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7053 - loss: 1.1929 - val_accuracy: 0.7184 - val_loss: 1.1312 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m48/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7281 - loss: 1.1043\n",
      "Epoch 19: val_loss improved from 1.13123 to 1.12490, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7267 - loss: 1.1108 - val_accuracy: 0.7156 - val_loss: 1.1249 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7190 - loss: 1.1280\n",
      "Epoch 20: val_loss improved from 1.12490 to 1.11974, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7189 - loss: 1.1303 - val_accuracy: 0.7240 - val_loss: 1.1197 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7138 - loss: 1.1661\n",
      "Epoch 21: val_loss improved from 1.11974 to 1.11217, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7152 - loss: 1.1560 - val_accuracy: 0.7195 - val_loss: 1.1122 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7216 - loss: 1.1170\n",
      "Epoch 22: val_loss improved from 1.11217 to 1.09598, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 1.1166 - val_accuracy: 0.7262 - val_loss: 1.0960 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7147 - loss: 1.1474\n",
      "Epoch 23: val_loss did not improve from 1.09598\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7171 - loss: 1.1412 - val_accuracy: 0.7234 - val_loss: 1.0961 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7281 - loss: 1.0872\n",
      "Epoch 24: val_loss improved from 1.09598 to 1.08486, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7268 - loss: 1.0940 - val_accuracy: 0.7295 - val_loss: 1.0849 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7321 - loss: 1.0954\n",
      "Epoch 25: val_loss did not improve from 1.08486\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7304 - loss: 1.0973 - val_accuracy: 0.7284 - val_loss: 1.0897 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7420 - loss: 1.0622\n",
      "Epoch 26: val_loss improved from 1.08486 to 1.07311, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7391 - loss: 1.0706 - val_accuracy: 0.7323 - val_loss: 1.0731 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7413 - loss: 1.0565\n",
      "Epoch 27: val_loss improved from 1.07311 to 1.06863, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7411 - loss: 1.0574 - val_accuracy: 0.7329 - val_loss: 1.0686 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7477 - loss: 1.0579\n",
      "Epoch 28: val_loss improved from 1.06863 to 1.06606, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7452 - loss: 1.0625 - val_accuracy: 0.7396 - val_loss: 1.0661 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7399 - loss: 1.0503\n",
      "Epoch 29: val_loss improved from 1.06606 to 1.06240, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7395 - loss: 1.0557 - val_accuracy: 0.7351 - val_loss: 1.0624 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 1.0245\n",
      "Epoch 30: val_loss improved from 1.06240 to 1.06076, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7430 - loss: 1.0338 - val_accuracy: 0.7357 - val_loss: 1.0608 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7293 - loss: 1.0861\n",
      "Epoch 31: val_loss improved from 1.06076 to 1.04958, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7298 - loss: 1.0849 - val_accuracy: 0.7385 - val_loss: 1.0496 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7339 - loss: 1.0511\n",
      "Epoch 32: val_loss improved from 1.04958 to 1.04874, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7339 - loss: 1.0512 - val_accuracy: 0.7362 - val_loss: 1.0487 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7253 - loss: 1.0999\n",
      "Epoch 33: val_loss improved from 1.04874 to 1.04236, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7286 - loss: 1.0877 - val_accuracy: 0.7435 - val_loss: 1.0424 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7400 - loss: 1.0432\n",
      "Epoch 34: val_loss improved from 1.04236 to 1.03819, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7404 - loss: 1.0436 - val_accuracy: 0.7435 - val_loss: 1.0382 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7343 - loss: 1.0360\n",
      "Epoch 35: val_loss did not improve from 1.03819\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7353 - loss: 1.0366 - val_accuracy: 0.7368 - val_loss: 1.0422 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7387 - loss: 1.0787\n",
      "Epoch 36: val_loss improved from 1.03819 to 1.03104, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7409 - loss: 1.0669 - val_accuracy: 0.7440 - val_loss: 1.0310 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7544 - loss: 1.0080\n",
      "Epoch 37: val_loss improved from 1.03104 to 1.03067, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7519 - loss: 1.0143 - val_accuracy: 0.7379 - val_loss: 1.0307 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - loss: 1.0269\n",
      "Epoch 38: val_loss did not improve from 1.03067\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7467 - loss: 1.0258 - val_accuracy: 0.7446 - val_loss: 1.0312 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 1.0043\n",
      "Epoch 39: val_loss did not improve from 1.03067\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7486 - loss: 1.0108 - val_accuracy: 0.7429 - val_loss: 1.0369 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7550 - loss: 1.0268\n",
      "Epoch 40: val_loss did not improve from 1.03067\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7535 - loss: 1.0239 - val_accuracy: 0.7396 - val_loss: 1.0328 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7522 - loss: 0.9931\n",
      "Epoch 41: val_loss improved from 1.03067 to 1.02487, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7517 - loss: 0.9989 - val_accuracy: 0.7474 - val_loss: 1.0249 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7407 - loss: 1.0122\n",
      "Epoch 42: val_loss improved from 1.02487 to 1.01243, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7424 - loss: 1.0140 - val_accuracy: 0.7485 - val_loss: 1.0124 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7530 - loss: 0.9861\n",
      "Epoch 43: val_loss did not improve from 1.01243\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7524 - loss: 0.9890 - val_accuracy: 0.7468 - val_loss: 1.0185 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 0.9755\n",
      "Epoch 44: val_loss did not improve from 1.01243\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7528 - loss: 0.9844 - val_accuracy: 0.7457 - val_loss: 1.0157 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7402 - loss: 1.0121\n",
      "Epoch 45: val_loss did not improve from 1.01243\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7409 - loss: 1.0131 - val_accuracy: 0.7490 - val_loss: 1.0176 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7540 - loss: 0.9758\n",
      "Epoch 46: val_loss did not improve from 1.01243\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7516 - loss: 0.9864 - val_accuracy: 0.7507 - val_loss: 1.0178 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7557 - loss: 0.9921\n",
      "Epoch 47: val_loss did not improve from 1.01243\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7543 - loss: 0.9945 - val_accuracy: 0.7462 - val_loss: 1.0146 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m51/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7474 - loss: 0.9843\n",
      "Epoch 48: val_loss improved from 1.01243 to 1.00597, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.9855 - val_accuracy: 0.7563 - val_loss: 1.0060 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m51/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7430 - loss: 1.0228\n",
      "Epoch 49: val_loss did not improve from 1.00597\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7439 - loss: 1.0201 - val_accuracy: 0.7496 - val_loss: 1.0064 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7493 - loss: 1.0130\n",
      "Epoch 50: val_loss did not improve from 1.00597\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7497 - loss: 1.0092 - val_accuracy: 0.7513 - val_loss: 1.0111 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m35/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7526 - loss: 0.9799\n",
      "Epoch 51: val_loss did not improve from 1.00597\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7528 - loss: 0.9838 - val_accuracy: 0.7479 - val_loss: 1.0076 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7563 - loss: 0.9822\n",
      "Epoch 52: val_loss improved from 1.00597 to 0.99608, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7551 - loss: 0.9834 - val_accuracy: 0.7540 - val_loss: 0.9961 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m35/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7520 - loss: 0.9902\n",
      "Epoch 53: val_loss did not improve from 0.99608\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7511 - loss: 0.9942 - val_accuracy: 0.7518 - val_loss: 0.9975 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m49/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7459 - loss: 1.0043\n",
      "Epoch 54: val_loss did not improve from 0.99608\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7467 - loss: 1.0027 - val_accuracy: 0.7507 - val_loss: 0.9996 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.9694\n",
      "Epoch 55: val_loss did not improve from 0.99608\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7600 - loss: 0.9710 - val_accuracy: 0.7513 - val_loss: 0.9995 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7489 - loss: 0.9977\n",
      "Epoch 56: val_loss improved from 0.99608 to 0.99532, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7501 - loss: 0.9915 - val_accuracy: 0.7485 - val_loss: 0.9953 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7533 - loss: 1.0004\n",
      "Epoch 57: val_loss improved from 0.99532 to 0.99233, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7531 - loss: 0.9955 - val_accuracy: 0.7518 - val_loss: 0.9923 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m51/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7405 - loss: 1.0076\n",
      "Epoch 58: val_loss did not improve from 0.99233\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7414 - loss: 1.0045 - val_accuracy: 0.7529 - val_loss: 0.9936 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7602 - loss: 0.9725\n",
      "Epoch 59: val_loss improved from 0.99233 to 0.98767, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7590 - loss: 0.9720 - val_accuracy: 0.7574 - val_loss: 0.9877 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m50/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7598 - loss: 0.9410\n",
      "Epoch 60: val_loss improved from 0.98767 to 0.98441, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7592 - loss: 0.9437 - val_accuracy: 0.7568 - val_loss: 0.9844 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7650 - loss: 0.9277\n",
      "Epoch 61: val_loss did not improve from 0.98441\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7620 - loss: 0.9382 - val_accuracy: 0.7490 - val_loss: 0.9965 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7543 - loss: 0.9738\n",
      "Epoch 62: val_loss improved from 0.98441 to 0.98355, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7540 - loss: 0.9739 - val_accuracy: 0.7540 - val_loss: 0.9836 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.9353\n",
      "Epoch 63: val_loss improved from 0.98355 to 0.98223, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7625 - loss: 0.9423 - val_accuracy: 0.7574 - val_loss: 0.9822 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7664 - loss: 0.9308\n",
      "Epoch 64: val_loss did not improve from 0.98223\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7663 - loss: 0.9314 - val_accuracy: 0.7579 - val_loss: 0.9910 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7512 - loss: 0.9640\n",
      "Epoch 65: val_loss did not improve from 0.98223\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7515 - loss: 0.9640 - val_accuracy: 0.7535 - val_loss: 0.9922 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m48/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7517 - loss: 0.9651\n",
      "Epoch 66: val_loss did not improve from 0.98223\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.9653 - val_accuracy: 0.7585 - val_loss: 0.9830 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7586 - loss: 0.9446\n",
      "Epoch 67: val_loss did not improve from 0.98223\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7584 - loss: 0.9467 - val_accuracy: 0.7551 - val_loss: 0.9888 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7569 - loss: 0.9504\n",
      "Epoch 68: val_loss did not improve from 0.98223\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7561 - loss: 0.9537 - val_accuracy: 0.7607 - val_loss: 0.9877 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7543 - loss: 0.9785\n",
      "Epoch 69: val_loss improved from 0.98223 to 0.98148, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7555 - loss: 0.9714 - val_accuracy: 0.7568 - val_loss: 0.9815 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7712 - loss: 0.9144\n",
      "Epoch 70: val_loss improved from 0.98148 to 0.97908, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7674 - loss: 0.9257 - val_accuracy: 0.7535 - val_loss: 0.9791 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.9180\n",
      "Epoch 71: val_loss did not improve from 0.97908\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7666 - loss: 0.9319 - val_accuracy: 0.7579 - val_loss: 0.9802 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.9229\n",
      "Epoch 72: val_loss did not improve from 0.97908\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7664 - loss: 0.9304 - val_accuracy: 0.7563 - val_loss: 0.9830 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.9490\n",
      "Epoch 73: val_loss did not improve from 0.97908\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7580 - loss: 0.9490 - val_accuracy: 0.7524 - val_loss: 0.9908 - learning_rate: 0.0010\n",
      "Epoch 74/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7635 - loss: 0.9233\n",
      "Epoch 74: val_loss did not improve from 0.97908\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7618 - loss: 0.9322 - val_accuracy: 0.7518 - val_loss: 0.9932 - learning_rate: 0.0010\n",
      "Epoch 75/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7405 - loss: 0.9778\n",
      "Epoch 75: val_loss did not improve from 0.97908\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7445 - loss: 0.9701 - val_accuracy: 0.7529 - val_loss: 0.9851 - learning_rate: 0.0010\n",
      "Epoch 76/200\n",
      "\u001b[1m47/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7539 - loss: 0.9472\n",
      "Epoch 76: val_loss improved from 0.97908 to 0.97598, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 0.9477 - val_accuracy: 0.7568 - val_loss: 0.9760 - learning_rate: 0.0010\n",
      "Epoch 77/200\n",
      "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7530 - loss: 0.9703\n",
      "Epoch 77: val_loss did not improve from 0.97598\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7532 - loss: 0.9693 - val_accuracy: 0.7602 - val_loss: 0.9776 - learning_rate: 0.0010\n",
      "Epoch 78/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7615 - loss: 0.9351\n",
      "Epoch 78: val_loss did not improve from 0.97598\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7613 - loss: 0.9369 - val_accuracy: 0.7551 - val_loss: 0.9827 - learning_rate: 0.0010\n",
      "Epoch 79/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.9577\n",
      "Epoch 79: val_loss did not improve from 0.97598\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7531 - loss: 0.9551 - val_accuracy: 0.7563 - val_loss: 0.9775 - learning_rate: 0.0010\n",
      "Epoch 80/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.9186\n",
      "Epoch 80: val_loss improved from 0.97598 to 0.97539, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7645 - loss: 0.9277 - val_accuracy: 0.7568 - val_loss: 0.9754 - learning_rate: 0.0010\n",
      "Epoch 81/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.9114\n",
      "Epoch 81: val_loss improved from 0.97539 to 0.97008, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7582 - loss: 0.9183 - val_accuracy: 0.7657 - val_loss: 0.9701 - learning_rate: 0.0010\n",
      "Epoch 82/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7698 - loss: 0.9224\n",
      "Epoch 82: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7679 - loss: 0.9272 - val_accuracy: 0.7602 - val_loss: 0.9723 - learning_rate: 0.0010\n",
      "Epoch 83/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7629 - loss: 0.9164\n",
      "Epoch 83: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7618 - loss: 0.9226 - val_accuracy: 0.7579 - val_loss: 0.9724 - learning_rate: 0.0010\n",
      "Epoch 84/200\n",
      "\u001b[1m47/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.9405\n",
      "Epoch 84: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7578 - loss: 0.9400 - val_accuracy: 0.7557 - val_loss: 0.9756 - learning_rate: 0.0010\n",
      "Epoch 85/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7646 - loss: 0.9242\n",
      "Epoch 85: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7638 - loss: 0.9276 - val_accuracy: 0.7563 - val_loss: 0.9828 - learning_rate: 0.0010\n",
      "Epoch 86/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.9109\n",
      "Epoch 86: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7664 - loss: 0.9184 - val_accuracy: 0.7563 - val_loss: 0.9733 - learning_rate: 0.0010\n",
      "Epoch 87/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7567 - loss: 0.9323\n",
      "Epoch 87: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7570 - loss: 0.9323 - val_accuracy: 0.7557 - val_loss: 0.9803 - learning_rate: 0.0010\n",
      "Epoch 88/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7469 - loss: 0.9735\n",
      "Epoch 88: val_loss did not improve from 0.97008\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7513 - loss: 0.9578 - val_accuracy: 0.7579 - val_loss: 0.9738 - learning_rate: 0.0010\n",
      "Epoch 89/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.9116\n",
      "Epoch 89: val_loss improved from 0.97008 to 0.96661, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.9165 - val_accuracy: 0.7585 - val_loss: 0.9666 - learning_rate: 0.0010\n",
      "Epoch 90/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7639 - loss: 0.9310\n",
      "Epoch 90: val_loss did not improve from 0.96661\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.9279 - val_accuracy: 0.7574 - val_loss: 0.9721 - learning_rate: 0.0010\n",
      "Epoch 91/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7709 - loss: 0.8998\n",
      "Epoch 91: val_loss improved from 0.96661 to 0.96424, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7687 - loss: 0.9074 - val_accuracy: 0.7579 - val_loss: 0.9642 - learning_rate: 0.0010\n",
      "Epoch 92/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7699 - loss: 0.8956\n",
      "Epoch 92: val_loss did not improve from 0.96424\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7686 - loss: 0.9012 - val_accuracy: 0.7618 - val_loss: 0.9669 - learning_rate: 0.0010\n",
      "Epoch 93/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.8856\n",
      "Epoch 93: val_loss did not improve from 0.96424\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7675 - loss: 0.8963 - val_accuracy: 0.7624 - val_loss: 0.9671 - learning_rate: 0.0010\n",
      "Epoch 94/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7685 - loss: 0.9191\n",
      "Epoch 94: val_loss improved from 0.96424 to 0.96180, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7683 - loss: 0.9192 - val_accuracy: 0.7618 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 95/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7624 - loss: 0.9509\n",
      "Epoch 95: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.9413 - val_accuracy: 0.7590 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 96/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7574 - loss: 0.9315\n",
      "Epoch 96: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7592 - loss: 0.9279 - val_accuracy: 0.7568 - val_loss: 0.9707 - learning_rate: 0.0010\n",
      "Epoch 97/200\n",
      "\u001b[1m51/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7659 - loss: 0.9157\n",
      "Epoch 97: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7659 - loss: 0.9154 - val_accuracy: 0.7635 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 98/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.9178\n",
      "Epoch 98: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7595 - loss: 0.9172 - val_accuracy: 0.7613 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 99/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.9286\n",
      "Epoch 99: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7617 - loss: 0.9254 - val_accuracy: 0.7590 - val_loss: 0.9680 - learning_rate: 0.0010\n",
      "Epoch 100/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.9194\n",
      "Epoch 100: val_loss did not improve from 0.96180\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7645 - loss: 0.9193 - val_accuracy: 0.7613 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 101/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7627 - loss: 0.9354\n",
      "Epoch 101: val_loss improved from 0.96180 to 0.96147, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7634 - loss: 0.9279 - val_accuracy: 0.7596 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 102/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.8662\n",
      "Epoch 102: val_loss did not improve from 0.96147\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7701 - loss: 0.8774 - val_accuracy: 0.7579 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 103/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7583 - loss: 0.9268\n",
      "Epoch 103: val_loss improved from 0.96147 to 0.96125, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7591 - loss: 0.9242 - val_accuracy: 0.7607 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 104/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7656 - loss: 0.9272\n",
      "Epoch 104: val_loss did not improve from 0.96125\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7663 - loss: 0.9221 - val_accuracy: 0.7641 - val_loss: 0.9664 - learning_rate: 0.0010\n",
      "Epoch 105/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.9110\n",
      "Epoch 105: val_loss did not improve from 0.96125\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7696 - loss: 0.9084 - val_accuracy: 0.7668 - val_loss: 0.9668 - learning_rate: 0.0010\n",
      "Epoch 106/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7621 - loss: 0.9098\n",
      "Epoch 106: val_loss did not improve from 0.96125\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7633 - loss: 0.9080 - val_accuracy: 0.7635 - val_loss: 0.9677 - learning_rate: 0.0010\n",
      "Epoch 107/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.9028\n",
      "Epoch 107: val_loss did not improve from 0.96125\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7718 - loss: 0.9036 - val_accuracy: 0.7657 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 108/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.9057\n",
      "Epoch 108: val_loss improved from 0.96125 to 0.95752, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7667 - loss: 0.9064 - val_accuracy: 0.7696 - val_loss: 0.9575 - learning_rate: 0.0010\n",
      "Epoch 109/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7672 - loss: 0.9059\n",
      "Epoch 109: val_loss improved from 0.95752 to 0.95339, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7688 - loss: 0.8992 - val_accuracy: 0.7685 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 110/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7589 - loss: 0.9187\n",
      "Epoch 110: val_loss did not improve from 0.95339\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7608 - loss: 0.9139 - val_accuracy: 0.7618 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 111/200\n",
      "\u001b[1m34/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7616 - loss: 0.9147\n",
      "Epoch 111: val_loss improved from 0.95339 to 0.95022, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.9080 - val_accuracy: 0.7657 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 112/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7723 - loss: 0.8959\n",
      "Epoch 112: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7716 - loss: 0.8978 - val_accuracy: 0.7641 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 113/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7672 - loss: 0.9056\n",
      "Epoch 113: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7675 - loss: 0.9028 - val_accuracy: 0.7657 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 114/200\n",
      "\u001b[1m47/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7819 - loss: 0.8415\n",
      "Epoch 114: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7797 - loss: 0.8506 - val_accuracy: 0.7641 - val_loss: 0.9585 - learning_rate: 0.0010\n",
      "Epoch 115/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7681 - loss: 0.8733\n",
      "Epoch 115: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7678 - loss: 0.8787 - val_accuracy: 0.7668 - val_loss: 0.9584 - learning_rate: 0.0010\n",
      "Epoch 116/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7690 - loss: 0.9103\n",
      "Epoch 116: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7695 - loss: 0.9067 - val_accuracy: 0.7657 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 117/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7689 - loss: 0.8963\n",
      "Epoch 117: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7696 - loss: 0.8945 - val_accuracy: 0.7629 - val_loss: 0.9604 - learning_rate: 0.0010\n",
      "Epoch 118/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.9022\n",
      "Epoch 118: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7697 - loss: 0.8973 - val_accuracy: 0.7657 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 119/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7767 - loss: 0.8666\n",
      "Epoch 119: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7753 - loss: 0.8712 - val_accuracy: 0.7663 - val_loss: 0.9651 - learning_rate: 0.0010\n",
      "Epoch 120/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7770 - loss: 0.8741\n",
      "Epoch 120: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7750 - loss: 0.8787 - val_accuracy: 0.7613 - val_loss: 0.9580 - learning_rate: 0.0010\n",
      "Epoch 121/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.8934\n",
      "Epoch 121: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7695 - loss: 0.8952 - val_accuracy: 0.7668 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 122/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7644 - loss: 0.8841\n",
      "Epoch 122: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7650 - loss: 0.8843 - val_accuracy: 0.7641 - val_loss: 0.9565 - learning_rate: 0.0010\n",
      "Epoch 123/200\n",
      "\u001b[1m48/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7685 - loss: 0.8888\n",
      "Epoch 123: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7686 - loss: 0.8880 - val_accuracy: 0.7691 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 124/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.8814\n",
      "Epoch 124: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7734 - loss: 0.8834 - val_accuracy: 0.7668 - val_loss: 0.9569 - learning_rate: 0.0010\n",
      "Epoch 125/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7699 - loss: 0.8961\n",
      "Epoch 125: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7700 - loss: 0.8955 - val_accuracy: 0.7674 - val_loss: 0.9596 - learning_rate: 0.0010\n",
      "Epoch 126/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7713 - loss: 0.8810\n",
      "Epoch 126: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7713 - loss: 0.8810 - val_accuracy: 0.7641 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 127/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.8601\n",
      "Epoch 127: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7740 - loss: 0.8657 - val_accuracy: 0.7624 - val_loss: 0.9556 - learning_rate: 0.0010\n",
      "Epoch 128/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7854 - loss: 0.8567\n",
      "Epoch 128: val_loss did not improve from 0.95022\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7845 - loss: 0.8601 - val_accuracy: 0.7657 - val_loss: 0.9652 - learning_rate: 0.0010\n",
      "Epoch 129/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7746 - loss: 0.8704\n",
      "Epoch 129: val_loss improved from 0.95022 to 0.94971, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7734 - loss: 0.8755 - val_accuracy: 0.7696 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 130/200\n",
      "\u001b[1m52/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7706 - loss: 0.8878\n",
      "Epoch 130: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7709 - loss: 0.8870 - val_accuracy: 0.7652 - val_loss: 0.9565 - learning_rate: 0.0010\n",
      "Epoch 131/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7802 - loss: 0.8681\n",
      "Epoch 131: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7784 - loss: 0.8700 - val_accuracy: 0.7691 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 132/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7660 - loss: 0.8969\n",
      "Epoch 132: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.8920 - val_accuracy: 0.7691 - val_loss: 0.9549 - learning_rate: 0.0010\n",
      "Epoch 133/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7837 - loss: 0.8645\n",
      "Epoch 133: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7811 - loss: 0.8665 - val_accuracy: 0.7718 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 134/200\n",
      "\u001b[1m50/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.8374\n",
      "Epoch 134: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7808 - loss: 0.8412 - val_accuracy: 0.7696 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 135/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.8375\n",
      "Epoch 135: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7800 - loss: 0.8428 - val_accuracy: 0.7696 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 136/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7664 - loss: 0.8844\n",
      "Epoch 136: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7677 - loss: 0.8831 - val_accuracy: 0.7663 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 137/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7735 - loss: 0.8748\n",
      "Epoch 137: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7741 - loss: 0.8749 - val_accuracy: 0.7652 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 138/200\n",
      "\u001b[1m35/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7713 - loss: 0.8882\n",
      "Epoch 138: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7728 - loss: 0.8833 - val_accuracy: 0.7652 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 139/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7849 - loss: 0.8293\n",
      "Epoch 139: val_loss did not improve from 0.94971\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7829 - loss: 0.8393 - val_accuracy: 0.7624 - val_loss: 0.9501 - learning_rate: 0.0010\n",
      "Epoch 140/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.8467\n",
      "Epoch 140: val_loss improved from 0.94971 to 0.94623, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7805 - loss: 0.8519 - val_accuracy: 0.7702 - val_loss: 0.9462 - learning_rate: 0.0010\n",
      "Epoch 141/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7758 - loss: 0.8589\n",
      "Epoch 141: val_loss did not improve from 0.94623\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7756 - loss: 0.8617 - val_accuracy: 0.7696 - val_loss: 0.9470 - learning_rate: 9.9104e-04\n",
      "Epoch 142/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7787 - loss: 0.8679\n",
      "Epoch 142: val_loss did not improve from 0.94623\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7784 - loss: 0.8680 - val_accuracy: 0.7691 - val_loss: 0.9500 - learning_rate: 9.8216e-04\n",
      "Epoch 143/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7741 - loss: 0.8775\n",
      "Epoch 143: val_loss improved from 0.94623 to 0.94583, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7750 - loss: 0.8739 - val_accuracy: 0.7685 - val_loss: 0.9458 - learning_rate: 9.7336e-04\n",
      "Epoch 144/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7600 - loss: 0.9168\n",
      "Epoch 144: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.9042 - val_accuracy: 0.7707 - val_loss: 0.9464 - learning_rate: 9.6464e-04\n",
      "Epoch 145/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7863 - loss: 0.8227\n",
      "Epoch 145: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7836 - loss: 0.8306 - val_accuracy: 0.7641 - val_loss: 0.9572 - learning_rate: 9.5600e-04\n",
      "Epoch 146/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7781 - loss: 0.8442\n",
      "Epoch 146: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7775 - loss: 0.8480 - val_accuracy: 0.7646 - val_loss: 0.9641 - learning_rate: 9.4743e-04\n",
      "Epoch 147/200\n",
      "\u001b[1m48/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7709 - loss: 0.8836\n",
      "Epoch 147: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7717 - loss: 0.8803 - val_accuracy: 0.7652 - val_loss: 0.9509 - learning_rate: 9.3894e-04\n",
      "Epoch 148/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.8759\n",
      "Epoch 148: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7715 - loss: 0.8704 - val_accuracy: 0.7668 - val_loss: 0.9576 - learning_rate: 9.3053e-04\n",
      "Epoch 149/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7829 - loss: 0.8299\n",
      "Epoch 149: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7815 - loss: 0.8365 - val_accuracy: 0.7746 - val_loss: 0.9555 - learning_rate: 9.2219e-04\n",
      "Epoch 150/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7782 - loss: 0.8391\n",
      "Epoch 150: val_loss did not improve from 0.94583\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7770 - loss: 0.8426 - val_accuracy: 0.7702 - val_loss: 0.9524 - learning_rate: 9.1393e-04\n",
      "Epoch 151/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7803 - loss: 0.8445\n",
      "Epoch 151: val_loss improved from 0.94583 to 0.94479, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7808 - loss: 0.8459 - val_accuracy: 0.7735 - val_loss: 0.9448 - learning_rate: 9.0574e-04\n",
      "Epoch 152/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.8730\n",
      "Epoch 152: val_loss did not improve from 0.94479\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7705 - loss: 0.8662 - val_accuracy: 0.7718 - val_loss: 0.9472 - learning_rate: 8.9763e-04\n",
      "Epoch 153/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7763 - loss: 0.8504\n",
      "Epoch 153: val_loss did not improve from 0.94479\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7760 - loss: 0.8523 - val_accuracy: 0.7663 - val_loss: 0.9521 - learning_rate: 8.8959e-04\n",
      "Epoch 154/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.8401\n",
      "Epoch 154: val_loss improved from 0.94479 to 0.94473, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7844 - loss: 0.8431 - val_accuracy: 0.7685 - val_loss: 0.9447 - learning_rate: 8.8162e-04\n",
      "Epoch 155/200\n",
      "\u001b[1m52/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.8745\n",
      "Epoch 155: val_loss did not improve from 0.94473\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7737 - loss: 0.8728 - val_accuracy: 0.7674 - val_loss: 0.9532 - learning_rate: 8.7372e-04\n",
      "Epoch 156/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7799 - loss: 0.8564\n",
      "Epoch 156: val_loss did not improve from 0.94473\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7799 - loss: 0.8562 - val_accuracy: 0.7674 - val_loss: 0.9511 - learning_rate: 8.6589e-04\n",
      "Epoch 157/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7793 - loss: 0.8341\n",
      "Epoch 157: val_loss did not improve from 0.94473\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7794 - loss: 0.8368 - val_accuracy: 0.7724 - val_loss: 0.9492 - learning_rate: 8.5813e-04\n",
      "Epoch 158/200\n",
      "\u001b[1m47/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7831 - loss: 0.8242\n",
      "Epoch 158: val_loss improved from 0.94473 to 0.94324, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7825 - loss: 0.8270 - val_accuracy: 0.7685 - val_loss: 0.9432 - learning_rate: 8.5044e-04\n",
      "Epoch 159/200\n",
      "\u001b[1m52/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7852 - loss: 0.8332\n",
      "Epoch 159: val_loss did not improve from 0.94324\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7850 - loss: 0.8337 - val_accuracy: 0.7724 - val_loss: 0.9460 - learning_rate: 8.4282e-04\n",
      "Epoch 160/200\n",
      "\u001b[1m49/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.8310\n",
      "Epoch 160: val_loss did not improve from 0.94324\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7824 - loss: 0.8320 - val_accuracy: 0.7696 - val_loss: 0.9462 - learning_rate: 8.3527e-04\n",
      "Epoch 161/200\n",
      "\u001b[1m52/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7704 - loss: 0.8660\n",
      "Epoch 161: val_loss did not improve from 0.94324\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7709 - loss: 0.8645 - val_accuracy: 0.7702 - val_loss: 0.9490 - learning_rate: 8.2779e-04\n",
      "Epoch 162/200\n",
      "\u001b[1m49/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7760 - loss: 0.8659\n",
      "Epoch 162: val_loss improved from 0.94324 to 0.94311, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7766 - loss: 0.8632 - val_accuracy: 0.7707 - val_loss: 0.9431 - learning_rate: 8.2037e-04\n",
      "Epoch 163/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7796 - loss: 0.8329\n",
      "Epoch 163: val_loss did not improve from 0.94311\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.8368 - val_accuracy: 0.7679 - val_loss: 0.9529 - learning_rate: 8.1302e-04\n",
      "Epoch 164/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7864 - loss: 0.8274\n",
      "Epoch 164: val_loss did not improve from 0.94311\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7854 - loss: 0.8296 - val_accuracy: 0.7691 - val_loss: 0.9488 - learning_rate: 8.0574e-04\n",
      "Epoch 165/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7908 - loss: 0.7841\n",
      "Epoch 165: val_loss improved from 0.94311 to 0.94115, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7874 - loss: 0.7981 - val_accuracy: 0.7741 - val_loss: 0.9411 - learning_rate: 7.9852e-04\n",
      "Epoch 166/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7872 - loss: 0.8050\n",
      "Epoch 166: val_loss did not improve from 0.94115\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7851 - loss: 0.8147 - val_accuracy: 0.7696 - val_loss: 0.9483 - learning_rate: 7.9136e-04\n",
      "Epoch 167/200\n",
      "\u001b[1m52/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.8315\n",
      "Epoch 167: val_loss did not improve from 0.94115\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7736 - loss: 0.8317 - val_accuracy: 0.7718 - val_loss: 0.9416 - learning_rate: 7.8427e-04\n",
      "Epoch 168/200\n",
      "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7782 - loss: 0.8550\n",
      "Epoch 168: val_loss did not improve from 0.94115\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7784 - loss: 0.8536 - val_accuracy: 0.7707 - val_loss: 0.9461 - learning_rate: 7.7725e-04\n",
      "Epoch 169/200\n",
      "\u001b[1m43/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.8460\n",
      "Epoch 169: val_loss improved from 0.94115 to 0.93758, saving model to best1.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7755 - loss: 0.8428 - val_accuracy: 0.7730 - val_loss: 0.9376 - learning_rate: 7.7028e-04\n",
      "Epoch 170/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7880 - loss: 0.8175\n",
      "Epoch 170: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7863 - loss: 0.8220 - val_accuracy: 0.7735 - val_loss: 0.9451 - learning_rate: 7.6338e-04\n",
      "Epoch 171/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7839 - loss: 0.8230\n",
      "Epoch 171: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7835 - loss: 0.8248 - val_accuracy: 0.7718 - val_loss: 0.9382 - learning_rate: 7.5654e-04\n",
      "Epoch 172/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7823 - loss: 0.8316\n",
      "Epoch 172: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7820 - loss: 0.8316 - val_accuracy: 0.7724 - val_loss: 0.9500 - learning_rate: 7.4976e-04\n",
      "Epoch 173/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7799 - loss: 0.8294\n",
      "Epoch 173: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7804 - loss: 0.8294 - val_accuracy: 0.7757 - val_loss: 0.9466 - learning_rate: 7.4304e-04\n",
      "Epoch 174/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7763 - loss: 0.8346\n",
      "Epoch 174: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7775 - loss: 0.8316 - val_accuracy: 0.7752 - val_loss: 0.9383 - learning_rate: 7.3639e-04\n",
      "Epoch 175/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7850 - loss: 0.8159\n",
      "Epoch 175: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7847 - loss: 0.8192 - val_accuracy: 0.7763 - val_loss: 0.9509 - learning_rate: 7.2979e-04\n",
      "Epoch 176/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.8362\n",
      "Epoch 176: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7812 - loss: 0.8302 - val_accuracy: 0.7679 - val_loss: 0.9518 - learning_rate: 7.2325e-04\n",
      "Epoch 177/200\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7663 - loss: 0.8809\n",
      "Epoch 177: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7711 - loss: 0.8632 - val_accuracy: 0.7718 - val_loss: 0.9421 - learning_rate: 7.1677e-04\n",
      "Epoch 178/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7811 - loss: 0.8486\n",
      "Epoch 178: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7819 - loss: 0.8449 - val_accuracy: 0.7774 - val_loss: 0.9414 - learning_rate: 7.1035e-04\n",
      "Epoch 179/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7894 - loss: 0.8184\n",
      "Epoch 179: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7878 - loss: 0.8206 - val_accuracy: 0.7691 - val_loss: 0.9465 - learning_rate: 7.0398e-04\n",
      "Epoch 180/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7915 - loss: 0.7898\n",
      "Epoch 180: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7903 - loss: 0.7960 - val_accuracy: 0.7718 - val_loss: 0.9484 - learning_rate: 6.9768e-04\n",
      "Epoch 181/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7781 - loss: 0.8329\n",
      "Epoch 181: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7799 - loss: 0.8289 - val_accuracy: 0.7685 - val_loss: 0.9384 - learning_rate: 6.9143e-04\n",
      "Epoch 182/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7899 - loss: 0.7931\n",
      "Epoch 182: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7884 - loss: 0.7993 - val_accuracy: 0.7702 - val_loss: 0.9500 - learning_rate: 6.8523e-04\n",
      "Epoch 183/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7851 - loss: 0.8065\n",
      "Epoch 183: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7860 - loss: 0.8088 - val_accuracy: 0.7718 - val_loss: 0.9427 - learning_rate: 6.7909e-04\n",
      "Epoch 184/200\n",
      "\u001b[1m40/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7818 - loss: 0.8135\n",
      "Epoch 184: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7828 - loss: 0.8155 - val_accuracy: 0.7718 - val_loss: 0.9436 - learning_rate: 6.7301e-04\n",
      "Epoch 185/200\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.8112\n",
      "Epoch 185: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7806 - loss: 0.8114 - val_accuracy: 0.7741 - val_loss: 0.9383 - learning_rate: 6.6698e-04\n",
      "Epoch 186/200\n",
      "\u001b[1m35/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7942 - loss: 0.7992\n",
      "Epoch 186: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7922 - loss: 0.8067 - val_accuracy: 0.7730 - val_loss: 0.9508 - learning_rate: 6.6100e-04\n",
      "Epoch 187/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7722 - loss: 0.8484\n",
      "Epoch 187: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7755 - loss: 0.8405 - val_accuracy: 0.7713 - val_loss: 0.9456 - learning_rate: 6.5508e-04\n",
      "Epoch 188/200\n",
      "\u001b[1m44/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7869 - loss: 0.8246\n",
      "Epoch 188: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7870 - loss: 0.8215 - val_accuracy: 0.7730 - val_loss: 0.9475 - learning_rate: 6.4921e-04\n",
      "Epoch 189/200\n",
      "\u001b[1m47/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.8387\n",
      "Epoch 189: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7836 - loss: 0.8352 - val_accuracy: 0.7774 - val_loss: 0.9513 - learning_rate: 6.4339e-04\n",
      "Epoch 190/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7892 - loss: 0.7991\n",
      "Epoch 190: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7888 - loss: 0.8041 - val_accuracy: 0.7707 - val_loss: 0.9438 - learning_rate: 6.3763e-04\n",
      "Epoch 191/200\n",
      "\u001b[1m46/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7831 - loss: 0.8306\n",
      "Epoch 191: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7836 - loss: 0.8273 - val_accuracy: 0.7724 - val_loss: 0.9424 - learning_rate: 6.3192e-04\n",
      "Epoch 192/200\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7864 - loss: 0.8268\n",
      "Epoch 192: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7868 - loss: 0.8233 - val_accuracy: 0.7718 - val_loss: 0.9456 - learning_rate: 6.2625e-04\n",
      "Epoch 193/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7816 - loss: 0.8290\n",
      "Epoch 193: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7830 - loss: 0.8234 - val_accuracy: 0.7735 - val_loss: 0.9511 - learning_rate: 6.2064e-04\n",
      "Epoch 194/200\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7835 - loss: 0.7913\n",
      "Epoch 194: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7830 - loss: 0.7974 - val_accuracy: 0.7707 - val_loss: 0.9401 - learning_rate: 6.1508e-04\n",
      "Epoch 195/200\n",
      "\u001b[1m37/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7905 - loss: 0.8052\n",
      "Epoch 195: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7900 - loss: 0.8087 - val_accuracy: 0.7735 - val_loss: 0.9460 - learning_rate: 6.0957e-04\n",
      "Epoch 196/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.8323\n",
      "Epoch 196: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.8274 - val_accuracy: 0.7730 - val_loss: 0.9444 - learning_rate: 6.0411e-04\n",
      "Epoch 197/200\n",
      "\u001b[1m45/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7839 - loss: 0.8226\n",
      "Epoch 197: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7842 - loss: 0.8214 - val_accuracy: 0.7724 - val_loss: 0.9420 - learning_rate: 5.9870e-04\n",
      "Epoch 198/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.8173\n",
      "Epoch 198: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7795 - loss: 0.8154 - val_accuracy: 0.7741 - val_loss: 0.9430 - learning_rate: 5.9333e-04\n",
      "Epoch 199/200\n",
      "\u001b[1m41/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7906 - loss: 0.7934\n",
      "Epoch 199: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7900 - loss: 0.7985 - val_accuracy: 0.7724 - val_loss: 0.9407 - learning_rate: 5.8802e-04\n",
      "Epoch 200/200\n",
      "\u001b[1m42/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7867 - loss: 0.8147\n",
      "Epoch 200: val_loss did not improve from 0.93758\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7867 - loss: 0.8139 - val_accuracy: 0.7718 - val_loss: 0.9449 - learning_rate: 5.8275e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "# Learning Rate Scheduler -- Change Learning Rate after 70% of epochs\n",
    "def learning_rate(epoch, lr):\n",
    "    if epoch < 0.7 * epochs:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.009))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(learning_rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=132, validation_data= (X_val, y_val), callbacks = [lr_scheduler,checkpoint ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
